{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "In this assignment you will train a neural network to predict the skill of a surgeon performing a fundamental task. The JHU-ISI Gesture and Skill Assessment Working Set [1] [2] is an open-source dataset collected at Johns Hopkins of trainee and expert surgeons performing basic tasks like knot tying, suturing and needle passing with the da Vinci robot[3]. <br>\n",
    "<br>\n",
    "\n",
    "In suturing, surgeons 'stitch up a wound' by passing a needle from one side of the tissue to the other. Check out the included videos to watch the surgeons perform a practice suturing task.\n",
    "<br> \n",
    "\n",
    "In this notebook you will be predicting the skill of the operating surgeon from the suturing kinematics and videos using a <b>2-layer, linear network</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "[1] Gao, Yixin, et al. \"Jhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset for human motion modeling.\" MICCAI Workshop: M2CAI. Vol. 3. 2014. <br>\n",
    "[2] https://cirl.lcsr.jhu.edu/research/hmm/datasets/jigsaws_release/ <br> \n",
    "[3] https://www.intuitive.com/en-us/products-and-services/da-vinci?gclid=Cj0KCQiAwP3yBRCkARIsAABGiPo79mPGJFNXWFc8tEpuRgU_s61N1zsmGR552MFbJ5C_LW12gXlG8AoaAmlIEALw_wcB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "In this assignment you will use: \n",
    "* os, sys for accessing files\n",
    "* pdb (optional) for debugging\n",
    "* NumPy for vectorized operations\n",
    "* matplotlib for plotting\n",
    "* mpl_toolkits for 3D plotting\n",
    "* utils_hw3_coding for helper functions\n",
    "* PyTorch (for implementing back propagation and neural networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from utils_hw3_coding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in JIGSAWS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "jigsaws_path = 'JIGSAW/'\n",
    "\n",
    "Train, Test = read_jigsaws_data(jigsaws_path)\n",
    "train_data = Train[0]; train_labels = Train[1]; train_files = Train[2]\n",
    "test_data  = Test[0];  test_labels  = Test[1];  test_files  = Test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(torch.nn.Module):\n",
    "    def __init__(self, window_size, hid_dim):        \n",
    "        super(LinearNet, self).__init__()\n",
    "        # TODO: fill in the correct linear layer sizes. \n",
    "        # Note, you may need to pass in parameter(s) when LinearNet is initialized\n",
    "        self.L1      = torch.nn.Linear(window_size*6, hid_dim)\n",
    "        self.L2      = torch.nn.Linear(hid_dim, 1)\n",
    "        self.ReLU    = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' forward pass '''\n",
    "        # TODO: implement the forward pass\n",
    "        x = self.ReLU(self.L1(x))\n",
    "        x = torch.sigmoid(self.L2(x))\n",
    "        return x\n",
    "#         pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        ''' predict labels 0/1 '''\n",
    "        # TODO: implement a predict function that predicts 0 or 1 for each example\n",
    "        x = self.forward(x)\n",
    "#         pass\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Set the Training Parameters '''\n",
    "batch_size  = 100 \n",
    "window_size = 100\n",
    "epochs      = 150\n",
    "learning_rate = 0.0001\n",
    "hid_dim     = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearNet(window_size, hid_dim)\n",
    "# TODO: choose a loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "# TODO: choose an optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0oss: 0.4860273599624634\n",
      "0.9090909090909091\n",
      "1oss: 0.5073659420013428\n",
      "2oss: 0.5196100473403931\n",
      "3oss: 0.5039699077606201\n",
      "4oss: 0.4883817732334137\n",
      "5oss: 0.5210750102996826\n",
      "6oss: 0.5174717903137207\n",
      "7oss: 0.4328964650630951\n",
      "8oss: 0.4629075527191162\n",
      "9oss: 0.45600631833076477\n",
      "10ss: 0.519481897354126\n",
      "0.9090909090909091\n",
      "11ss: 0.45604920387268066\n",
      "12ss: 0.47404271364212036\n",
      "13ss: 0.5161123871803284\n",
      "14ss: 0.48916545510292053\n",
      "15ss: 0.49720609188079834\n",
      "16ss: 0.4163954555988312\n",
      "17ss: 0.5283619165420532\n",
      "18ss: 0.4896260201931\n",
      "19ss: 0.5084235072135925\n",
      "20ss: 0.41672104597091675\n",
      "0.9090909090909091\n",
      "21ss: 0.38872870802879333\n",
      "22ss: 0.4947454035282135\n",
      "23ss: 0.4482777714729309\n",
      "24ss: 0.5077322125434875\n",
      "25ss: 0.5124091506004333\n",
      "26ss: 0.4053930938243866\n",
      "27ss: 0.435245543718338\n",
      "28ss: 0.4217585623264313\n",
      "29ss: 0.4081770181655884\n",
      "30ss: 0.5277518630027771\n",
      "0.9090909090909091\n",
      "31ss: 0.4432532489299774\n",
      "32ss: 0.5160080194473267\n",
      "33ss: 0.5343470573425293\n",
      "34ss: 0.49491870403289795\n",
      "35ss: 0.4469638466835022\n",
      "36ss: 0.407416433095932\n",
      "37ss: 0.4260982871055603\n",
      "38ss: 0.5134603977203369\n",
      "39ss: 0.5308806896209717\n",
      "40ss: 0.4861045777797699\n",
      "0.9090909090909091\n",
      "41ss: 0.46337389945983887\n",
      "42ss: 0.5490604043006897\n",
      "43ss: 0.4347507059574127\n",
      "44ss: 0.490843266248703\n",
      "45ss: 0.5034628510475159\n",
      "46ss: 0.4604155719280243\n",
      "47ss: 0.5510985851287842\n",
      "48ss: 0.46859291195869446\n",
      "49ss: 0.42451077699661255\n",
      "50ss: 0.42193669080734253\n",
      "0.9090909090909091\n",
      "51ss: 0.35391104221343994\n",
      "52ss: 0.5348801016807556\n",
      "53ss: 0.4635103940963745\n",
      "54ss: 0.42965930700302124\n",
      "55ss: 0.4904667139053345\n",
      "56ss: 0.40744805335998535\n",
      "57ss: 0.46730637550354004\n",
      "58ss: 0.38501980900764465\n",
      "59ss: 0.4426560699939728\n",
      "60ss: 0.49770310521125793\n",
      "0.9090909090909091\n",
      "61ss: 0.4193632900714874\n",
      "62ss: 0.4174513518810272\n",
      "63ss: 0.4794219732284546\n",
      "64ss: 0.44449591636657715\n",
      "65ss: 0.37045562267303467\n",
      "66ss: 0.5687252283096313\n",
      "67ss: 0.3639043867588043\n",
      "68ss: 0.5263276100158691\n",
      "69ss: 0.42056944966316223\n",
      "70ss: 0.3905944526195526\n",
      "0.9090909090909091\n",
      "71ss: 0.4524105489253998\n",
      "72ss: 0.4382457733154297\n",
      "73ss: 0.5084171891212463\n",
      "74ss: 0.42273110151290894\n",
      "75ss: 0.38334599137306213\n",
      "76ss: 0.49049633741378784\n",
      "77ss: 0.47013550996780396\n",
      "78ss: 0.40503621101379395\n",
      "79ss: 0.4679824113845825\n",
      "80ss: 0.4588841199874878\n",
      "0.9090909090909091\n",
      "81ss: 0.3655887544155121\n",
      "82ss: 0.5088828802108765\n",
      "83ss: 0.4620051681995392\n",
      "84ss: 0.42132335901260376\n",
      "85ss: 0.47173306345939636\n",
      "86ss: 0.44343671202659607\n",
      "87ss: 0.4736635088920593\n",
      "88ss: 0.503716230392456\n",
      "89ss: 0.45181429386138916\n",
      "90ss: 0.46031492948532104\n",
      "0.9090909090909091\n",
      "91ss: 0.4416261911392212\n",
      "92ss: 0.38760149478912354\n",
      "93ss: 0.4405519366264343\n",
      "94ss: 0.4478643536567688\n",
      "95ss: 0.4400140345096588\n",
      "96ss: 0.41045334935188293\n",
      "97ss: 0.42495861649513245\n",
      "98ss: 0.47865790128707886\n",
      "99ss: 0.4635895788669586\n",
      "100s: 0.46750178933143616\n",
      "0.9090909090909091\n",
      "101s: 0.4452030062675476\n",
      "102s: 0.47354450821876526\n",
      "103s: 0.5149535536766052\n",
      "104s: 0.42956334352493286\n",
      "105s: 0.45855778455734253\n",
      "106s: 0.5165521502494812\n",
      "107s: 0.45789965987205505\n",
      "108s: 0.40326255559921265\n",
      "109s: 0.3887057602405548\n",
      "110s: 0.4676610827445984\n",
      "0.9090909090909091\n",
      "111s: 0.4504639804363251\n",
      "112s: 0.40938857197761536\n",
      "113s: 0.37487220764160156\n",
      "114s: 0.45015212893486023\n",
      "115s: 0.48491737246513367\n",
      "116s: 0.3587248623371124\n",
      "117s: 0.4276231527328491\n",
      "118s: 0.42912426590919495\n",
      "119s: 0.3847680687904358\n",
      "120s: 0.4565974771976471\n",
      "0.9090909090909091\n",
      "121s: 0.47409844398498535\n",
      "122s: 0.4190900921821594\n",
      "123s: 0.4319576323032379\n",
      "124s: 0.3283316493034363\n",
      "125s: 0.4770353436470032\n",
      "126s: 0.5332919359207153\n",
      "127s: 0.48386189341545105\n",
      "128s: 0.47248420119285583\n",
      "129s: 0.4512869119644165\n",
      "130s: 0.41093555092811584\n",
      "0.9090909090909091\n",
      "131s: 0.49116408824920654\n",
      "132s: 0.39462754130363464\n",
      "133s: 0.5072357058525085\n",
      "134s: 0.4423260986804962\n",
      "135s: 0.48114001750946045\n",
      "136s: 0.43163996934890747\n",
      "137s: 0.44869503378868103\n",
      "138s: 0.4474974572658539\n",
      "139s: 0.4356101155281067\n",
      "140s: 0.4973757863044739\n",
      "0.9090909090909091\n",
      "141s: 0.5299878120422363\n",
      "142s: 0.43859291076660156\n",
      "143s: 0.4190099239349365\n",
      "144s: 0.3181242346763611\n",
      "145s: 0.3549689054489136\n",
      "146s: 0.5140541195869446\n",
      "147s: 0.4554087817668915\n",
      "148s: 0.47185036540031433\n",
      "149s: 0.4058782458305359\n"
     ]
    }
   ],
   "source": [
    "''' Training Loop  '''\n",
    "Loss = []\n",
    "Train_Accuracy = []\n",
    "Test_Accuracy  = []\n",
    "\n",
    "for epic in range(epochs):\n",
    "    model.train()\n",
    "    window, window_labels = get_window(train_data, train_labels, window_size, batch_size)\n",
    "    \n",
    "    ''' ...... '''\n",
    "    # TODO: implement training on your network, compute the loss, update weights\n",
    "    window = torch.from_numpy(window).float()\n",
    "    window_labels = torch.from_numpy(window_labels).float()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(window)\n",
    "    loss = loss_fn(out, window_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ''' ...... '''\n",
    "    \n",
    "\n",
    "    Loss.append(loss.detach().item())\n",
    "    print(\"Loss: \" + str(loss.detach().item()), end='\\r')\n",
    "    print(epic)\n",
    "    \n",
    "    if(epic % 10 == 0):\n",
    "        ''' We have implemented the function compute_accuracy to help you evaluate your network. \n",
    "        It returns the accuracy of the binary skill predictions.'''\n",
    "        train_accuracy, train_predictions = compute_accuracy(train_data, train_labels, model, window_size)\n",
    "        test_accuracy,  test_predictions  = compute_accuracy(test_data, test_labels, model, window_size)\n",
    "        Train_Accuracy.append(train_accuracy)\n",
    "        Test_Accuracy.append(test_accuracy)  \n",
    "        print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8571428571428571\n",
      "Testing Accuracy: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "''' Print out the model accuracy after training '''\n",
    "train_accuracy, train_predictions = compute_accuracy(train_data, train_labels, model, window_size)\n",
    "test_accuracy,  test_predictions  = compute_accuracy(test_data, test_labels, model, window_size)\n",
    "\n",
    "print(\"Training Accuracy: \" + str(train_accuracy))\n",
    "print(\"Testing Accuracy: \" + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-36b6d6aeb76a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# plot the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "''' Evaluate your Network '''\n",
    "\n",
    "# plot the loss \n",
    "fig = plt.figure()\n",
    "plt.plot(Loss, label=\"Loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Visualize Accuracy '''\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(Train_Accuracy, label=\"Train_Accuracy\")\n",
    "plt.plot(Test_Accuracy, label=\"Test_Accuracy\")\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Questions\n",
    "* How does the learning rate impact training? Try training networks with 3 different learning rates. Explain the impact you observe of learning rate on training? <br><br>\n",
    "<i> Your response here. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many epochs are required for your network to \"converge\"? How can you tell if the network has converged? How does the time to convergence relate to the learning rate? Refer back to the 3 different learning rates you tried in the question above. <br><br>\n",
    "<i> Your response here. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In your LinearNet, you project the input data into some smaller dimension (`size_2`). Does changing `size_2` change the performance of your network? Explain using 3 different dimensions you tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
